[2023-04-05 15:30:27,561::train::INFO] Namespace(beta_1=0.0001, beta_T=0.02, categories=['airplane'], dataset_path='./data/shapenet.hdf5', device='cuda', end_lr=0.0001, flexibility=0.0, kl_weight=0.001, latent_dim=256, latent_flow_depth=14, latent_flow_hidden_dim=256, log_root='./logs_gen', logging=True, lr=0.002, max_grad_norm=10, max_iters=inf, model='flow', num_samples=4, num_steps=100, residual=True, sample_num_points=2048, scale_mode='shape_unit', sched_end_epoch=400000, sched_mode='linear', sched_start_epoch=200000, seed=2020, spectral_norm=False, tag=None, test_freq=30000, test_size=400, train_batch_size=128, truncate_std=2.0, val_batch_size=64, val_freq=1000, weight_decay=0)
[2023-04-05 15:30:27,562::train::INFO] Loading datasets...
[2023-04-05 15:30:28,342::train::INFO] Building model...
[2023-04-05 15:30:30,857::train::INFO] FlowVAE(
  (encoder): PointNetEncoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc1_m): Linear(in_features=512, out_features=256, bias=True)
    (fc2_m): Linear(in_features=256, out_features=128, bias=True)
    (fc3_m): Linear(in_features=128, out_features=256, bias=True)
    (fc_bn1_m): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc_bn2_m): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc1_v): Linear(in_features=512, out_features=256, bias=True)
    (fc2_v): Linear(in_features=256, out_features=128, bias=True)
    (fc3_v): Linear(in_features=128, out_features=256, bias=True)
    (fc_bn1_v): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc_bn2_v): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (flow): SequentialFlow(
    (chain): ModuleList(
      (0): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (1): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (2): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (3): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (4): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (5): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (6): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (7): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (8): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (9): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (10): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (11): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (12): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (13): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (diffusion): DiffusionPoint(
    (net): PointwiseNet(
      (layers): ModuleList(
        (0): ConcatSquashLinear(
          (_layer): Linear(in_features=3, out_features=128, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)
        )
        (1): ConcatSquashLinear(
          (_layer): Linear(in_features=128, out_features=256, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)
        )
        (2): ConcatSquashLinear(
          (_layer): Linear(in_features=256, out_features=512, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=512, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=512, bias=True)
        )
        (3): ConcatSquashLinear(
          (_layer): Linear(in_features=512, out_features=256, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)
        )
        (4): ConcatSquashLinear(
          (_layer): Linear(in_features=256, out_features=128, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)
        )
        (5): ConcatSquashLinear(
          (_layer): Linear(in_features=128, out_features=3, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=3, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=3, bias=True)
        )
      )
    )
    (var_sched): VarianceSchedule()
  )
)
[2023-04-05 15:30:30,858::train::INFO] Start training...
[2023-04-05 15:30:33,734::train::INFO] [Train] Iter 0001 | Loss 61.608116 | Grad 3.7477 | KLWeight 0.0010
[2023-04-05 15:30:33,871::train::INFO] [Train] Iter 0002 | Loss 61.707367 | Grad 3.7796 | KLWeight 0.0010
[2023-04-05 15:30:34,006::train::INFO] [Train] Iter 0003 | Loss 61.562263 | Grad 2.4302 | KLWeight 0.0010
[2023-04-05 15:30:34,138::train::INFO] [Train] Iter 0004 | Loss 61.398674 | Grad 1.6968 | KLWeight 0.0010
[2023-04-05 15:30:34,271::train::INFO] [Train] Iter 0005 | Loss 61.374943 | Grad 1.2413 | KLWeight 0.0010
[2023-04-05 15:30:34,403::train::INFO] [Train] Iter 0006 | Loss 61.427086 | Grad 1.7131 | KLWeight 0.0010
[2023-04-05 15:30:34,541::train::INFO] [Train] Iter 0007 | Loss 61.466904 | Grad 1.8625 | KLWeight 0.0010
[2023-04-05 15:30:34,675::train::INFO] [Train] Iter 0008 | Loss 61.250546 | Grad 1.2762 | KLWeight 0.0010
[2023-04-05 15:30:34,809::train::INFO] [Train] Iter 0009 | Loss 61.238106 | Grad 1.0550 | KLWeight 0.0010
[2023-04-05 15:30:34,947::train::INFO] [Train] Iter 0010 | Loss 61.247360 | Grad 0.8595 | KLWeight 0.0010
[2023-04-05 15:30:35,080::train::INFO] [Train] Iter 0011 | Loss 61.197124 | Grad 0.8229 | KLWeight 0.0010
[2023-04-05 15:30:35,214::train::INFO] [Train] Iter 0012 | Loss 61.271904 | Grad 0.9259 | KLWeight 0.0010
[2023-04-05 15:30:35,351::train::INFO] [Train] Iter 0013 | Loss 61.235882 | Grad 0.8036 | KLWeight 0.0010
[2023-04-05 15:30:35,495::train::INFO] [Train] Iter 0014 | Loss 61.191250 | Grad 0.6694 | KLWeight 0.0010
[2023-04-05 15:30:35,629::train::INFO] [Train] Iter 0015 | Loss 61.157925 | Grad 0.6657 | KLWeight 0.0010
[2023-04-05 15:30:35,772::train::INFO] [Train] Iter 0016 | Loss 61.066185 | Grad 0.6142 | KLWeight 0.0010
[2023-04-05 15:30:35,905::train::INFO] [Train] Iter 0017 | Loss 61.133427 | Grad 0.4685 | KLWeight 0.0010
[2023-04-05 15:30:36,046::train::INFO] [Train] Iter 0018 | Loss 61.058746 | Grad 0.3930 | KLWeight 0.0010
[2023-04-05 15:30:36,181::train::INFO] [Train] Iter 0019 | Loss 61.031887 | Grad 0.4524 | KLWeight 0.0010
[2023-04-05 15:30:36,314::train::INFO] [Train] Iter 0020 | Loss 60.975487 | Grad 0.6374 | KLWeight 0.0010
[2023-04-05 15:30:36,453::train::INFO] [Train] Iter 0021 | Loss 60.902332 | Grad 0.7569 | KLWeight 0.0010
[2023-04-05 15:30:36,585::train::INFO] [Train] Iter 0022 | Loss 60.882305 | Grad 0.8242 | KLWeight 0.0010
[2023-04-05 15:30:36,716::train::INFO] [Train] Iter 0023 | Loss 60.911381 | Grad 1.6148 | KLWeight 0.0010
[2023-04-05 15:30:36,847::train::INFO] [Train] Iter 0024 | Loss 60.896240 | Grad 0.7398 | KLWeight 0.0010
[2023-04-05 15:30:36,977::train::INFO] [Train] Iter 0025 | Loss 60.880768 | Grad 0.4309 | KLWeight 0.0010
[2023-04-05 15:30:37,107::train::INFO] [Train] Iter 0026 | Loss 60.947304 | Grad 0.4747 | KLWeight 0.0010
[2023-04-05 15:30:37,223::train::INFO] [Train] Iter 0027 | Loss 60.891708 | Grad 0.4802 | KLWeight 0.0010
[2023-04-05 15:30:37,353::train::INFO] [Train] Iter 0028 | Loss 60.907299 | Grad 0.5200 | KLWeight 0.0010
[2023-04-05 15:30:37,485::train::INFO] [Train] Iter 0029 | Loss 60.866257 | Grad 0.5552 | KLWeight 0.0010
[2023-04-05 15:30:37,615::train::INFO] [Train] Iter 0030 | Loss 60.834156 | Grad 0.5030 | KLWeight 0.0010
[2023-04-05 15:30:37,745::train::INFO] [Train] Iter 0031 | Loss 60.855812 | Grad 0.3930 | KLWeight 0.0010
[2023-04-05 15:30:37,876::train::INFO] [Train] Iter 0032 | Loss 60.880775 | Grad 0.6053 | KLWeight 0.0010
[2023-04-05 15:30:38,007::train::INFO] [Train] Iter 0033 | Loss 60.790806 | Grad 0.7764 | KLWeight 0.0010
[2023-04-05 15:30:38,062::train::INFO] Terminating...
