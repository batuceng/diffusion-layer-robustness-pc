[2023-04-05 15:30:49,001::train::INFO] Namespace(beta_1=0.0001, beta_T=0.02, categories=['airplane'], dataset_path='./data/shapenet.hdf5', device='cuda', end_lr=0.0001, flexibility=0.0, kl_weight=0.001, latent_dim=256, latent_flow_depth=14, latent_flow_hidden_dim=256, log_root='./logs_gen', logging=True, lr=0.002, max_grad_norm=10, max_iters=inf, model='flow', num_samples=4, num_steps=100, residual=True, sample_num_points=2048, scale_mode='shape_unit', sched_end_epoch=400000, sched_mode='linear', sched_start_epoch=200000, seed=2020, spectral_norm=False, tag=None, test_freq=30000, test_size=400, train_batch_size=128, truncate_std=2.0, val_batch_size=64, val_freq=1000, weight_decay=0)
[2023-04-05 15:30:49,001::train::INFO] Loading datasets...
[2023-04-05 15:30:49,701::train::INFO] Building model...
[2023-04-05 15:30:50,611::train::INFO] FlowVAE(
  (encoder): PointNetEncoder(
    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
    (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
    (conv3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
    (conv4): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc1_m): Linear(in_features=512, out_features=256, bias=True)
    (fc2_m): Linear(in_features=256, out_features=128, bias=True)
    (fc3_m): Linear(in_features=128, out_features=256, bias=True)
    (fc_bn1_m): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc_bn2_m): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc1_v): Linear(in_features=512, out_features=256, bias=True)
    (fc2_v): Linear(in_features=256, out_features=128, bias=True)
    (fc3_v): Linear(in_features=128, out_features=256, bias=True)
    (fc_bn1_v): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc_bn2_v): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (flow): SequentialFlow(
    (chain): ModuleList(
      (0): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (1): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (2): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (3): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (4): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (5): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (6): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (7): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (8): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (9): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (10): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (11): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (12): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (13): CouplingLayer(
        (net_s_t): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU(inplace=True)
          (4): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (diffusion): DiffusionPoint(
    (net): PointwiseNet(
      (layers): ModuleList(
        (0): ConcatSquashLinear(
          (_layer): Linear(in_features=3, out_features=128, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)
        )
        (1): ConcatSquashLinear(
          (_layer): Linear(in_features=128, out_features=256, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)
        )
        (2): ConcatSquashLinear(
          (_layer): Linear(in_features=256, out_features=512, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=512, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=512, bias=True)
        )
        (3): ConcatSquashLinear(
          (_layer): Linear(in_features=512, out_features=256, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)
        )
        (4): ConcatSquashLinear(
          (_layer): Linear(in_features=256, out_features=128, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)
        )
        (5): ConcatSquashLinear(
          (_layer): Linear(in_features=128, out_features=3, bias=True)
          (_hyper_bias): Linear(in_features=259, out_features=3, bias=False)
          (_hyper_gate): Linear(in_features=259, out_features=3, bias=True)
        )
      )
    )
    (var_sched): VarianceSchedule()
  )
)
[2023-04-05 15:30:50,611::train::INFO] Start training...
[2023-04-05 15:30:52,801::train::INFO] [Train] Iter 0001 | Loss 61.608116 | Grad 3.7477 | KLWeight 0.0010
[2023-04-05 15:30:52,933::train::INFO] [Train] Iter 0002 | Loss 61.707367 | Grad 3.7795 | KLWeight 0.0010
[2023-04-05 15:30:53,074::train::INFO] [Train] Iter 0003 | Loss 61.562260 | Grad 2.4326 | KLWeight 0.0010
[2023-04-05 15:30:53,215::train::INFO] [Train] Iter 0004 | Loss 61.398998 | Grad 1.6897 | KLWeight 0.0010
[2023-04-05 15:30:53,356::train::INFO] [Train] Iter 0005 | Loss 61.374565 | Grad 1.2370 | KLWeight 0.0010
[2023-04-05 15:30:53,496::train::INFO] [Train] Iter 0006 | Loss 61.428642 | Grad 1.7148 | KLWeight 0.0010
[2023-04-05 15:30:53,635::train::INFO] [Train] Iter 0007 | Loss 61.466705 | Grad 1.8646 | KLWeight 0.0010
[2023-04-05 15:30:53,775::train::INFO] [Train] Iter 0008 | Loss 61.253174 | Grad 1.2951 | KLWeight 0.0010
[2023-04-05 15:30:53,908::train::INFO] [Train] Iter 0009 | Loss 61.238541 | Grad 1.0708 | KLWeight 0.0010
[2023-04-05 15:30:54,040::train::INFO] [Train] Iter 0010 | Loss 61.249149 | Grad 0.8661 | KLWeight 0.0010
[2023-04-05 15:30:54,172::train::INFO] [Train] Iter 0011 | Loss 61.194267 | Grad 0.8202 | KLWeight 0.0010
[2023-04-05 15:30:54,304::train::INFO] [Train] Iter 0012 | Loss 61.271965 | Grad 0.9408 | KLWeight 0.0010
[2023-04-05 15:30:54,434::train::INFO] [Train] Iter 0013 | Loss 61.236252 | Grad 0.8261 | KLWeight 0.0010
[2023-04-05 15:30:54,574::train::INFO] [Train] Iter 0014 | Loss 61.191963 | Grad 0.6811 | KLWeight 0.0010
[2023-04-05 15:30:54,715::train::INFO] [Train] Iter 0015 | Loss 61.156490 | Grad 0.6492 | KLWeight 0.0010
[2023-04-05 15:30:54,856::train::INFO] [Train] Iter 0016 | Loss 61.063549 | Grad 0.5858 | KLWeight 0.0010
[2023-04-05 15:30:54,997::train::INFO] [Train] Iter 0017 | Loss 61.131668 | Grad 0.4561 | KLWeight 0.0010
[2023-04-05 15:30:55,142::train::INFO] [Train] Iter 0018 | Loss 61.057308 | Grad 0.3958 | KLWeight 0.0010
[2023-04-05 15:30:55,283::train::INFO] [Train] Iter 0019 | Loss 61.029751 | Grad 0.4690 | KLWeight 0.0010
[2023-04-05 15:30:55,423::train::INFO] [Train] Iter 0020 | Loss 60.970577 | Grad 0.6246 | KLWeight 0.0010
[2023-04-05 15:30:55,557::train::INFO] [Train] Iter 0021 | Loss 60.895958 | Grad 0.7718 | KLWeight 0.0010
[2023-04-05 15:30:55,689::train::INFO] [Train] Iter 0022 | Loss 60.891197 | Grad 1.0482 | KLWeight 0.0010
[2023-04-05 15:30:55,823::train::INFO] [Train] Iter 0023 | Loss 60.894089 | Grad 1.4677 | KLWeight 0.0010
[2023-04-05 15:30:55,880::train::INFO] Terminating...
